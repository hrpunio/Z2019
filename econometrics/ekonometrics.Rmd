---
title: "Making ekonometrics more meaningful (RR approach)"
author: "Tomasz Przechlewski"
date: "25 pa≈∫dziernika 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Reproducible research idea
    
Abandoning the habit of secrecy in favor of process transparency and
peer review was the crucial step by which alchemy became chemistry.
Eric S. Raymond, E. S. The art of UNIX programming: Addison-Wesley.

Replicability vs Reproducibility

Hot topic: google: reproducible research = 158000

**Replicability**: independent experiment targetting the same question
will produce a result consistent with the original study.
  
**Reproducibility**: ability to repeat
the experiment with exactly the same outcome as
originally reported [description of method/code/data is needed to do so].

Computational science is facing a credibility crisis: it's impossible
to verify most of the computational results presented at conferences
and in papers today. (Donoho D. et al 2009)

## Australopithecus (Current practices)

* Enter data in Excel/OOCalc to clean and/or make explanatory analysis.

  + Use Excel for data cleaning & descriptive statistics

  + Excel handles missing data inconsistently and sometimes incorrectly

  + Many common functions are poor or missing in Excel

* Import data from Spreadsheet into SPSS/SAS/Stata for serious analysis

* Use SPSS/SAS/Stata in point-and-click mode to run serious
statistical analyses.

* Prepare report/paper: copy and paste output to Word/OpenOffice, add
description.

* Send to publisher (repeat 1--4 if returned for revision).
  
Problems

* Tedious/time-wasting/costly.

* Even small data/method change requires
extensive recomputation effort/careful report/paper revision and update.

* Error-prone: difficult to record/remember a 'click history'.

Famous example: Reinhart and Rogoff controversy
Countries very high GDP--debt ratio suffer from low growth. However the study
suffers serious but easy identifiable flaws which were discovered when
RR published the dataset they used in their analysis
(cf [Growth_in_a_Time_of_Debt](https://en.wikipedia.org/wiki/Growth_in_a_Time_of_Debt))
  
## Homo habilis (Enhanced current practices)

* Abandon spreadsheets.

* Abandon point-and-click mode. Use statistical scripting
languages and run program/scripts.

Benefits

Improved: reliability, transparency, automation, maintanability.
Lower costs (in the long run).

Solves 1--2 but not 3--4.

Problems: Steeper learning curve.
Perhaps higher costs in short run.
Duplication of effort (or mess if scripts/programs are poorly documented).

## Homo Erectus (Literate statistical programming)

Literate programming concept:
Code and description in one document. Create software as
works of literature, by embedding source code inside
descriptive text, rather than the reverse (as in most programming
languages), in an order that is convenient for human readers.

A program is like a WEB tangled and weaved (turned into a document),
with relations and connections in the program parts.  We express a
program as a *web of ideas*.  WEB is a combination of
-- a document formatting language and -- a program language.

General idea of Literate statistical programming mimics Knuth's WEB system.

Statistical computing code is embedded inside descriptive
text. Literate statistical program is weaved (turned) into
report/paper by executing code and inserting the results
obtained. data/method changes.

Solves 1--4.

## LSP: Benefits/Problems/Tools

* Reliability: Easier to find/fix bugs.
The results produced will not change when recomputed (in theory at least).

* Efficiency: Reuse allows to avoid duplication of effort (Payoff in the long run.)

* Transparency: increased citation rate, broader impact, improved institutional memory

* Institutional memory is a collective set of facts, concepts, experiences and know-how 
held by a group of people. 

* Flexibility: When you don't 'point-and-click' you gain many new analytic options.

Problems of LSP: Many incl. costs and learning curve

Tools:

* Document formatting language: LaTeX (not recommended) or Markdown (or
many others, ie. orgmode).  LaTeX is a word processor/a document
markup language.  Markdown: lightweight document markup language based
on email text formatting. Easy to write, read and publish as-is.

* Program language: R

## Interlude: Github for the uninitiated

The basic idea is that instead of manually registerging 
changes one has made to data, documents etc, one can use software to
help him manage the whole process.
Such software is called **Version Control Systems** or VCS

VCS not only manages content, registering each modification of it, but 
control access to the content as well. Thus many individuals can
work on common project (compare this to common scenario of mailing
spreadsheets to each other--higly inefficient at least)

There are highly reliable and publicly available VCS services
and GitHub is the most popular of them.

GitHub is owned by Microsoft (do not use if you boycott MS :-))

I use GitHub as an educational tool: to distribute learning content
to my students and to store content they produce for me (ie projects)

The free GitHub account is public. It is OK for me. If it is not OK
for you, you can buy a license for commercial account or
do not use GitHub.

## Linear Regression of the Janka Hardness Data

First, we load the data into a dataframe from CSV file:
```{r}
janka = read.csv("janka.csv", header = TRUE, sep=";")
```

The `header=TRUE` option tells R to use the first line as column names
Try typing just `janka` into the command line to see the raw data:
```{r}
janka
```

A scatterplot of the data is simple in R
```{r}
plot(janka, main = "Hardness vs. Density of Timber")
```

This is the same as calling the following command (but notice the axes are labeled different)
```{r}
plot(janka$hardness ~ janka$dens, main = "Hardness vs. Density of Timber")
```

The `y ~ x` means y is the dependent variable and x is the independent variable.
The `lm` command stands for "linear model" and will fit a linear regression to
paired data. It uses the same `y ~ x` syntax.
```{r}
janka.model = lm(hardness ~ dens, data = janka)
```

The output of `lm` is an object with lots of information about the linear
regression fit. Type the following command to see what variables it contains.
```{r}
names(janka.model)
```

The first item to look at is the "coefficients" this is the intercept (alpha)
and slope (beta)
```{r}
janka.model$coefficients
```

The command "abline" will add a line to the plot with a certain intercept a
(first parameter) and slope b (second parameter)
```{r}
plot(janka$hardness ~ janka$dens, main = "Hardness vs. Density of Timber")
abline(janka.model$coefficients[1], janka.model$coefficients[2], col = "red", lwd = 3)
```

The abline command is smart enough to know when you pass it a regression fit.
The above command can be simplified like so:
```{r}
plot(janka$hardness ~ janka$dens, main = "Hardness vs. Density of Timber")
abline(janka.model, col = "red", lwd = 3)
```

Hypothesis testing of the intercept and slope parameters is easy in R. Here the null hypothesis is that the intercept (or respectively, slope) is equal to zero. The alternative hypothesis is that the intercept (respectively, slope) is not equal to zero. The p-values you see in the table are for these two hypotheses.
```{r}
summary(janka.model)
```


The next thing we can look at is the residuals between the data and the model
For the hypothesis testing assumptions to be valid, these should be roughly
Gaussian:
```{r}
hist(janka.model$residuals, main = "Residuals of Janka Regression")
```

Another way to visualize if the residuals seem Gaussian distributed is a
Q-Q plot, which stands for "Quantile-Quantile" plot. It plots the empirical
quantiles of the data (y axis) versus the theoretical quantiles of a
Gaussian distribution (x axis). If the data comes from a Gaussian, the
points should lie on a 45 degree line. See the Wikipedia page for more info:

https://en.wikipedia.org/wiki/Q-Q_plot

```{r}
qqnorm(janka.model$residuals / sd(janka.model$residuals))
abline(0, 1)
```

Here we plot the residual values vs. the independent variable. According to
our regression assumptions, there shouldn't be a trend in these residuals.
That is, the residuals should be i.i.d. regardless of the X value.
```{r}
plot(janka.model$residuals ~ janka$hardness, main = "Residuals vs. Hardness")
```


## Another example: Malbork Castle

First, we load the data into a dataframe:

```{r}
dane <- read.csv(file= "MZM.csv", header=T, sep=";");
```

Inspect the data

```{r}
str(dane)

# show top
head(dane)

# show length
length(dane)
```

Load required library (`forecast`).
Transform data (column) on total number of visitors
into object of type ts (time-series)

```{r}
library(forecast)

visitors <- ts(dane$razem, frequency=12, start = c(2015, 01), end = c(2019, 03))
# Plot the data
plot(visitors)

# show start/end/frequency of data
start(visitors)
end(visitors)
frequency(visitors)

## test/learn sets (not used)
visitors.learn <- window (visitors, end = c(2018, 1))
start (visitors.learn)
end(visitors.learn)
length(visitors.learn)

visitors.test <- window(visitors, start= c(2019, 1))
start(visitors.test)
end(visitors.test)
length(visitors.test)
```

Linear trend

```{r}
mzm.trend <- tslm(visitors ~ trend)

summary(mzm.trend )

plot(visitors, main="Linear trend")
lines(fitted(mzm.trend), col="blue", lty=2)
legend("topleft", legend=c("observed", "trend"), col=c("black", "blue"), lty=c(1,2))

tsdisplay(residuals(mzm.trend), main="residuals")

plot(forecast(mzm.trend, h=3))
```

Linear trend with seasonality

```{r}
mzm.trend.season <- tslm(visitors ~ trend + season)

summary(mzm.trend.season )

plot(visitors, main="Linear trend", col="black")
lines(fitted(mzm.trend.season), col="blue", lty=2)
legend("topleft", legend=c("observed", "trend"), col=c("black", "blue"), lty=c(1,2))

tsdisplay(residuals(mzm.trend.season), main="residuals")

plot(forecast(mzm.trend.season, h=3))
```

Squared trend with seasonality

```{r}
mzm.trend.season.sq <- tslm(visitors ~ season + poly(trend, raw=T, degree=2), lambda=0)
summary(mzm.trend.season.sq)
```

End of story

## Summary: New Tools 

* R/Rstudio for computing and data visualization

* Github for enhancing team work

* markdown for reproducible research

**Learnig resources**

* [Rstudio](https://www.rstudio.com/resources/cheatsheets/)

* [Polish Main Statistical Office](https://stat.gov.pl/)

* [Bank Danych Lokalnych (Local Data Bank)]()https://bdl.stat.gov.pl/BDL/start)

* [Eurostat (European Union Statistical Office)](https://ec.europa.eu/eurostat)

* [My github repository](https://github.com/hrpunio))

* [Tourism](http://appsso.eurostat.ec.europa.eu/nui/submitViewTableAction.do)

* [Coffee production/consumption](https://www.kaggle.com/sbajew/icos-crop-data)

* [Econometrics](https://justinmshea.github.io/wooldridge/)

* [ScPoEconometrics](https://github.com/ScPoEcon/ScPoEconometrics)